
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Cross-Context Visual Imitation Learning from Demonstrations</title>
    <!-- CSS -->
        <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Oxygen:400,300' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/style.css" type="text/css" media="screen" />
    <!-- ENDS CSS -->

        <!--script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');

        </script-->

        <!-- ENDS JS -->  
  </head> 
  <body>
    <!-- MAIN -->
    <div id="main">

            <div id="match-nav-wrapper">
                <div id="match-nav-bar">

                    <table>
                        <thead>
                            <tr>
                                <th width="11%"><a href="#">Home</a></th>
                                <th width="13%"><a href="#demo-video">Video</a></th>
                                <th width="16%"><a href="#data-code">Data & Code</a></th>
                                <th width="60%"></th>
                            </tr>
                        </thead>
                    </table>

                </div>
            </div>

      <!-- HEADER -->
      <div id="home-header">
      
            <div id="match-wrapper">

                <h5><a href="#"><img src="image/logo.png"></a></h5>

                <br><br>

                <h2>Zero Real-Interaction Imitation Learning with Image-only Observation</h2>
                <center>
                  Xuanhui Xu<sup>1</sup>, Mingyu You<sup>1*</sup>, Hongjun Zhou<sup>1</sup>, Zhifeng Qian<sup>2</sup>, and Bin He<sup>1</sup><br>
<font size=2><sup>1</sup>School of Control Science and Engineering, Shandong University&nbsp;&nbsp;<sup>2</sup>Department of Automation, Shanghai Jiao Tong University</font>                
                </center>
                <br><strong>Abstract</strong>
                <p style="text-align:justify; text-justify:inter-ideograph;">
                Learning from observation (LfO) prompts the robot to imitate actions from state-only demonstrations via deep Reinforcement Learning (RL). LfO has achieved satisfactory results in simulation environment through hundreds of thousands of robot-environment interactions.
                 While in the real world, constrained by the expensive and even potentially dangerous interaction between the real robot and the environment, LfO is still difficult to be popularized. Reducing the number of the interaction of LfO during training in the real world remains to be a hot research topic. Although significant progress has been made in existing works, interaction is still inevitable. In this paper, we propose the LION net ($\textbf{L}earning\ from\ \textbf{I}mage-only\ \textbf{O}bservatio\textbf{n}\ Net$) which learns action from image-only demonstrations, and reduce the number of robot-environment interaction to zero in the real world. It is expected to be a realistic solution for the real robot LfO. LION net contains two modules: a domain transfer module which bridges the gap between the simulation and the real world and a RL based control module that takes images as input. With LION net, the robot is allowed to imitate the action from the image-only human demonstration in simulation, and perform the learned action in the real world without any additional training. We evaluate our method on three real life tasks: pouring water, washing cup, and stacking cube. All tasks are deployed to the real robot and achieved state-of-the-art performance.
</p>
                <br>
                <h1>Overview</h1>
                <center><img src="image/overview.png" width="60%"></center>
                <p style="text-align:justify; text-justify:inter-ideograph;">
                 We proposed a cross-context visual imitation learning method, which consists of three models: context translation model that translates the observation image to the context of robot from a different context; depth prediction model that generates predictive depth observation to provide depth input for the inverse model; multi-modal inverse dynamics model that maps the multi-modal observations into actions to reproduce the behaviours in demonstrations.</p>
              
                <div id="demo-video"></div>
                <br><br><h1>Video</h1>
                <iframe width="880" height="495" src="https://www.youtube.com/embed/5z5C8prTRmg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

                <div id="data-code"></div>
                <br><br><h1>Data & Code</h1>
                The data and code will be released soon!
                <br><br><br>
              </div>
      <!-- ENDS Footer -->
  </body>
</html>
